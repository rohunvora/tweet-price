name: Hourly Data Update

on:
  schedule:
    # Run every hour at minute 5 (avoid exactly on the hour when many jobs run)
    - cron: '5 * * * *'
  workflow_dispatch:
    # Allow manual trigger from GitHub UI

env:
  PYTHONUNBUFFERED: '1'  # Real-time output for all Python scripts

jobs:
  update-data:
    runs-on: ubuntu-latest

    permissions:
      contents: write  # Needed to push commits

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Restore database from cache
        uses: actions/cache@v4
        id: cache-db
        with:
          path: data/analytics.duckdb
          key: analytics-db-${{ github.run_id }}
          restore-keys: |
            analytics-db-

      - name: Create data directory
        run: mkdir -p data

      - name: Check if database exists (from cache)
        id: check-db
        run: |
          if [ -f "data/analytics.duckdb" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            SIZE=$(du -h data/analytics.duckdb | cut -f1)
            echo "‚úÖ Database found in cache ($SIZE)"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Database not found in cache"
          fi

      - name: Download database from release (if not cached)
        if: steps.check-db.outputs.exists == 'false'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üì• Downloading database from release..."
          if gh release download database --pattern "analytics.duckdb" --dir data/ 2>/dev/null; then
            SIZE=$(du -h data/analytics.duckdb | cut -f1)
            echo "‚úÖ Downloaded database from release ($SIZE)"
          else
            echo "‚ùå No database release found!"
            echo "To bootstrap: create a release tagged 'database' with analytics.duckdb attached"
            exit 1
          fi

      - name: Verify database and show status
        id: verify-db
        run: |
          cd scripts
          python -c "
import sys
from db import get_connection
import json
from datetime import datetime, timezone

print('=' * 60)
print('DATABASE STATUS BEFORE UPDATE')
print('=' * 60)
print(f'Current time: {datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M UTC\")}')
print()

conn = get_connection()

with open('assets.json') as f:
    assets = [a for a in json.load(f)['assets'] if a.get('enabled', True)]

all_ok = True
for asset in assets:
    asset_id = asset['id']

    tweet_result = conn.execute('''
        SELECT MAX(timestamp) as latest, COUNT(*) as total
        FROM tweets WHERE asset_id = ?
    ''', [asset_id]).fetchone()

    price_result = conn.execute('''
        SELECT MAX(timestamp) as latest, COUNT(*) as total
        FROM prices WHERE asset_id = ? AND timeframe = '1h'
    ''', [asset_id]).fetchone()

    tweet_latest = tweet_result[0] if tweet_result[0] else 'none'
    price_latest = price_result[0] if price_result[0] else 'none'

    print(f'{asset_id.upper():10} | Tweets: {tweet_result[1]:4} (latest: {tweet_latest})')
    print(f'           | Prices: {price_result[1]:5} 1h candles (latest: {price_latest})')

    if price_result[1] == 0:
        print(f'           | ‚ö†Ô∏è  WARNING: No price data!')
        all_ok = False
    print()

conn.close()

if not all_ok:
    print('‚ö†Ô∏è  Some assets have missing data - fetch may take longer')
else:
    print('‚úÖ All assets have data')
"
          echo "db_verified=true" >> $GITHUB_OUTPUT

      - name: Fetch new tweets
        env:
          X_BEARER_TOKEN: ${{ secrets.X_BEARER_TOKEN }}
        run: |
          echo "üê¶ Fetching new tweets..."
          cd scripts
          python fetch_tweets.py 2>&1 || echo "‚ö†Ô∏è Tweet fetch had errors (continuing anyway)"
        continue-on-error: true
        timeout-minutes: 10

      - name: Fetch new prices (15m, 1h, 1d)
        env:
          BIRDEYE_API_KEY: ${{ secrets.BIRDEYE_API_KEY }}
        run: |
          echo "üìä Fetching new prices (15m, 1h, 1d timeframes)..."
          cd scripts
          # Fetch 15m, 1h, 1d for hourly updates - skip 1m (too slow, removed from UI)
          python fetch_prices.py -t 15m -t 1h -t 1d 2>&1 || echo "‚ö†Ô∏è Price fetch had errors (continuing anyway)"
        continue-on-error: true
        timeout-minutes: 15

      - name: Compute statistics
        run: |
          echo "üìà Computing statistics..."
          cd scripts
          python compute_stats.py 2>&1 || echo "‚ö†Ô∏è Stats computation had errors"
        continue-on-error: true
        timeout-minutes: 5

      - name: Export static data
        id: export
        run: |
          echo "üì¶ Exporting static JSON files..."
          cd scripts
          python export_static.py 2>&1

          # Verify export produced files
          if [ -d "../web/public/static/pump" ]; then
            echo "‚úÖ Export completed"
            echo "export_ok=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Export failed - no output files!"
            echo "export_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
        timeout-minutes: 5

      - name: Validate exported data
        if: steps.export.outputs.export_ok == 'true'
        run: |
          echo "üîç Validating exported data..."
          cd scripts
          python -c "
import json
import os
from pathlib import Path

static_dir = Path('../web/public/static')
errors = []

with open('assets.json') as f:
    assets = [a for a in json.load(f)['assets'] if a.get('enabled', True)]

print()
for asset in assets:
    asset_id = asset['id']
    asset_dir = static_dir / asset_id

    # Check required files exist
    required = ['tweet_events.json', 'prices_1h.json', 'prices_1d.json', 'stats.json']
    missing = [f for f in required if not (asset_dir / f).exists()]

    if missing:
        errors.append(f'{asset_id}: missing {missing}')
        print(f'‚ùå {asset_id.upper()}: missing {missing}')
        continue

    # Check tweet_events has data
    with open(asset_dir / 'tweet_events.json') as f:
        events = json.load(f)
        tweet_count = events.get('count', 0)

    # Check prices have data
    with open(asset_dir / 'prices_1h.json') as f:
        prices = json.load(f)
        price_count = prices.get('count', 0)

    if tweet_count == 0:
        errors.append(f'{asset_id}: no tweets')
        print(f'‚ö†Ô∏è  {asset_id.upper()}: {tweet_count} tweets, {price_count} price candles')
    elif price_count == 0:
        errors.append(f'{asset_id}: no prices')
        print(f'‚ùå {asset_id.upper()}: {tweet_count} tweets, {price_count} price candles')
    else:
        print(f'‚úÖ {asset_id.upper()}: {tweet_count} tweets, {price_count} price candles')

print()
if errors:
    print(f'‚ö†Ô∏è  Validation warnings: {len(errors)}')
    for e in errors:
        print(f'   - {e}')
else:
    print('‚úÖ All assets validated successfully')
"

      - name: Generate timestamp
        run: |
          TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          echo "{\"timestamp\": \"$TIMESTAMP\", \"source\": \"github-actions\"}" > web/public/static/last_updated.json
          echo "‚è∞ Timestamp: $TIMESTAMP"

      - name: Check for changes
        id: check-changes
        run: |
          git add -A
          if git diff --cached --quiet; then
            echo "changed=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  No changes to commit"
          else
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "üìù Changes detected:"
            git diff --cached --stat | head -20
          fi

      - name: Commit and push changes
        if: steps.check-changes.outputs.changed == 'true'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M UTC")
          git commit -m "chore: hourly data update $TIMESTAMP"
          git push
          echo "‚úÖ Changes pushed to repository"

      - name: Save database to cache
        uses: actions/cache/save@v4
        with:
          path: data/analytics.duckdb
          key: analytics-db-${{ github.run_id }}

      - name: Upload database to release (backup)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üíæ Backing up database to release..."
          gh release upload database data/analytics.duckdb --clobber 2>/dev/null || \
            gh release create database data/analytics.duckdb --title "Database Backup" --notes "Auto-updated by hourly workflow"
          echo "‚úÖ Database backed up"

      - name: Summary
        run: |
          echo ""
          echo "========================================"
          echo "           WORKFLOW COMPLETE"
          echo "========================================"
          if [ "${{ steps.check-changes.outputs.changed }}" == "true" ]; then
            echo "‚úÖ Data updated and pushed"
            echo "üöÄ Vercel will auto-deploy shortly"
          else
            echo "‚ÑπÔ∏è  No new data to update"
          fi
          echo "========================================"
